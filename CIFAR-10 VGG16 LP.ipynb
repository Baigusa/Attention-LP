{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
    "import tempfile\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np \n",
    "from keras.layers import Conv2D,Conv1D,Reshape,MaxPooling2D,Multiply,GlobalAveragePooling2D,Multiply,Lambda, Activation,GlobalMaxPooling2D,Dense,Input\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "import pickle\n",
    "#from tensorflow.keras.layers.experimental import GroupNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9c3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ca027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.layers import GroupNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588617c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#------------------------------------------------------------------------------\n",
    "#  Loading CIFAR10 data\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(\"******************\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afd4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices using one hot encoding\n",
    "y_train_ohe = to_categorical(y_train, num_classes = 10)\n",
    "y_test_ohe = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85007409",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bce8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "x_train=tf.image.resize(x_train,(48,48))\n",
    "x_test=tf.image.resize(x_test,(48,48))\n",
    "x_train = x_train.numpy()\n",
    "x_test = x_test.numpy()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train  /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print(\"******************\")\n",
    "print(x_train.shape)\n",
    "print(y_train_ohe.shape)\n",
    "print(x_test.shape) \n",
    "print(y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c13b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29335f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[40000:]\n",
    "y_val = y_train_ohe[40000:]\n",
    "print(x_val.shape)  \n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c52a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:40000]\n",
    "y_train_ohe = y_train_ohe[:40000]\n",
    "print(x_train.shape)\n",
    "print(y_train_ohe.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = VGG16(weights='imagenet',\n",
    "                    include_top=False, \n",
    "                    classes=10,\n",
    "                    input_shape=(48,48,3)# input: images with 3 channels -> (224,224,3) tensors.\n",
    "                   )\n",
    "\n",
    "#Define the sequential model and add th VGG's layers to it\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "for layer in vgg16_model.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flops_count(new_model):\n",
    "    total_flops = 0\n",
    "    for layer in new_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            #print(f'{layer.name}') \n",
    "            kernel_size = layer.kernel_size\n",
    "            input_channels = layer.input_shape[-1]\n",
    "            output_channels = layer.output_shape[-1]\n",
    "            height = layer.output_shape[1]\n",
    "            width = layer.output_shape[2]\n",
    "            flops = ((kernel_size[0] * kernel_size[1] * input_channels) * output_channels + output_channels) * height * width\n",
    "            total_flops += flops\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            input_neurons = layer.input_shape[-1]\n",
    "            output_neurons = layer.units\n",
    "            flops = (input_neurons * output_neurons) + output_neurons\n",
    "            total_flops += flops\n",
    "    return total_flops\n",
    "print(f'Model Flops: {flops_count(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# TRAINING THE CNN ON THE TRAIN/VALIDATION DATA\n",
    "#------------------------------------------------------------------------------\n",
    "from tensorflow.keras import optimizers\n",
    "# initiate SGD optimizer\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * (0.1 ** (epoch // 40))\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "#mc = ModelCheckpoint('./weights.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "Monitor=[\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=10),\n",
    "    tf.keras.callbacks.ModelCheckpoint('model.h5',monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "]\n",
    "\n",
    "# initialize the number of epochs and batch size\n",
    "EPOCHS = 100\n",
    "BS = 32\n",
    "\n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20, \n",
    "    zoom_range=0.15, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True, \n",
    "    fill_mode=\"nearest\")\n",
    " \n",
    "# train the model\n",
    "history = model.fit_generator(aug.flow(x_train,y_train_ohe, batch_size=BS,seed=42),validation_data=(x_val,y_val),\n",
    "    steps_per_epoch=len(x_train) // BS,epochs=EPOCHS,callbacks=[reduce_lr,Monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbcac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss, accuracy = model.evaluate(x_test, y_test_ohe, verbose=1)\n",
    "print('Baseline test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228817e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training weights for future use\n",
    "#model.save_weights('cifar_100_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the training weights for future use\n",
    "#model.save_weights('model_cifar10.ckpt')\n",
    "model.save_weights('model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71957784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('model.ckpt')\n",
    "#model.load_weights('model_cifar100_512.ckpt')\n",
    "model.load_weights('model_cifar10.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a21c2",
   "metadata": {},
   "source": [
    "# Inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b46c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Prepare a sample input image for inference (adjust shape as needed)\n",
    "input_image = x_test[0:1]  # Taking the first image for inference\n",
    "\n",
    "# Warm-up the model (optional)\n",
    "model.predict(input_image)\n",
    "\n",
    "#input_image = x_test[0:100]\n",
    "\n",
    "# Measure inference latency\n",
    "n = 10000  # Number of repetitions\n",
    "total_time = 0\n",
    "\n",
    "for i in range(n):\n",
    "    start_time = time.time()\n",
    "    model.predict(x_test[i:i+1])\n",
    "    end_time = time.time()\n",
    "    total_time += (end_time - start_time)\n",
    "\n",
    "average_latency = (total_time / n) *1000\n",
    "print(\"Average Latency:\", average_latency, \"Milli seconds\")\n",
    "print(\"Total time:\", total_time, \" seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47126f58",
   "metadata": {},
   "source": [
    "# LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd6ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AVGSTDGNFilterPruning(model):\n",
    "    \n",
    "    class GlobalStdPooling(tf.keras.layers.Layer):\n",
    "        def call(self, inputs):\n",
    "            # Calculate global average pooling\n",
    "            mean = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "            #max = tf.reduce_max(inputs, axis=[1, 2], keepdims=True) \n",
    "            #print('mean:')\n",
    "            #tf.print(max)\n",
    "            # Subtract the mean from each channel\n",
    "            centered_inputs = inputs - mean\n",
    "\n",
    "            # Square the centered differences\n",
    "            squared_diff = tf.square(centered_inputs)\n",
    "            #print(\"squared_diff:\")\n",
    "            #print(  squared_diff.shape)\n",
    "            # Calculate the standard deviation\n",
    "            std_dev = (tf.sqrt(tf.reduce_mean(squared_diff, axis=[1, 2], keepdims=True)+1e-5)) #epsilon=1e-5\n",
    "            #print('std:')\n",
    "            #tf.print(std_dev)\n",
    "            # Flatten the std_dev tensor\n",
    "            flattened_mean = tf.keras.layers.Flatten()( mean )\n",
    "            channel_std = tf.keras.layers.Flatten()(std_dev)\n",
    "            #tf.print(flattened_mean)\n",
    "            #tf.print(flattened_mean-2 )\n",
    "\n",
    "            return  channel_std\n",
    "    \n",
    "    \n",
    "    def cbam(inputs, ratio=16):\n",
    "        # Create ECA block\n",
    "        x=inputs\n",
    "        p = GlobalAveragePooling2D()(x) \n",
    "        q = GlobalStdPooling()(x) \n",
    "        x1=p+q\n",
    "        #x1=Reshape((x.shape[-1],1))(x1)\n",
    "        #x1 = Dense(units=x.shape[-1] // 16, activation='relu')(x1)\n",
    "        #x1=Conv1D(filters=1,kernel_size=3,padding='same',activation='sigmoid')(x1)\n",
    "        #x1 = Dense(units=x.shape[-1], activation='relu')(x1)\n",
    "        #x1=Reshape((x.shape[-1],))(x1)\n",
    "        x1=GroupNormalization(groups=4)(x1)\n",
    "\n",
    "        x2 = GlobalMaxPooling2D()(x)\n",
    "        #x2 = GlobalStdPooling()(x)\n",
    "        #x2=m+n\n",
    "        #x2=Reshape((x.shape[-1],1))(x2) \n",
    "        #x2=Conv1D(filters=1,kernel_size=3,padding='same',activation='sigmoid')(x2)\n",
    "        #x2 = Dense(units=x.shape[-1] // 16, activation='relu')(x2)\n",
    "        #x2 = Dense(units=x.shape[-1], activation='relu')(x2)\n",
    "        #x2=Reshape((x.shape[-1],))(x2)\n",
    "        #x2=GroupNormalization(groups=4)(x2)\n",
    "        #x2 = GlobalStdPooling()(x)\n",
    "        x2=GroupNormalization(groups=4)(x2)\n",
    "        \n",
    "        \n",
    "        features=x1+x2\n",
    "        features=Activation(\"sigmoid\")(features)\n",
    "\n",
    "        excitation = Reshape((1, 1, x.shape[-1]))(features)\n",
    "        scale = Multiply()([x, excitation]) \n",
    "        return scale\n",
    "\n",
    "    def build_vgg16_CBAM():\n",
    "\n",
    "        # Create a new model using the functional API\n",
    "        inputs = tf.keras.Input(shape=(48, 48, 3))\n",
    "        x = inputs\n",
    "        for layer in model.layers:       # Except 1st layer\n",
    "            if isinstance(layer, layers.Conv2D):\n",
    "                x = layer(x)\n",
    "                x = cbam(x)  # Apply SE block after each Conv2D layer\n",
    "            if isinstance(layer, MaxPooling2D):\n",
    "                x = layer(x)\n",
    "\n",
    "        # Classification layers\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        x = layers.Dense(512, activation='relu')(x)\n",
    "        outputs = layers.Dense(10, activation='softmax')(x)  # CIFAR-10 has 10 classes\n",
    "\n",
    "        new_model = tf.keras.Model(inputs, outputs)\n",
    "        return new_model\n",
    "    new_model = build_vgg16_CBAM()\n",
    "\n",
    "    # Copy the weights layer by layer\n",
    "    #for layer1, layer2 in zip(cbam_model.layers, new_model.layers):\n",
    "       # weights = layer1.get_weights()\n",
    "       # layer2.set_weights(weights)\n",
    "\n",
    "    # copy Conv2D layer weight\n",
    "    conv2D_index = [0, 1, 3, 4, 6, 7, 8, 10, 11, 12, 14, 15, 16]  # List of indices for Conv2D layers in the new model\n",
    "\n",
    "    conv_index = 0  # Index variable to keep track of Conv2D layers in the new model\n",
    "    for layer in new_model.layers:\n",
    "        if isinstance(layer, Conv2D):\n",
    "            model_layer = model.layers[conv2D_index[conv_index]]\n",
    "            layer.set_weights(model_layer.get_weights())\n",
    "            conv_index += 1\n",
    "\n",
    "    # copy dense layer weight\n",
    "    trained_flatten_index = None\n",
    "    new_flatten_index = None\n",
    "\n",
    "    for index, layer in enumerate(model.layers, start=0):\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            trained_flatten_index = index\n",
    "            break\n",
    "\n",
    "    for index, layer in enumerate(new_model.layers, start=0):\n",
    "        if isinstance(layer, tf.keras.layers.Flatten):\n",
    "            new_flatten_index = index\n",
    "            break\n",
    "\n",
    "    if trained_flatten_index is not None and new_flatten_index is not None:\n",
    "        trained_dense_layers = model.layers[trained_flatten_index + 1:trained_flatten_index + 4]\n",
    "        new_dense_layers = new_model.layers[new_flatten_index + 1:new_flatten_index + 4]\n",
    "\n",
    "        for trained_dense_layer, new_dense_layer in zip(trained_dense_layers, new_dense_layers):\n",
    "            new_dense_layer.set_weights(trained_dense_layer.get_weights())\n",
    "    else:\n",
    "        print(\"Flatten layer not found in one or both models.\")\n",
    "\n",
    "    # initiate SGD optimizer\n",
    "    sgd = optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        return 0.001 * (0.5 ** (epoch // 15))\n",
    "    reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # new model retraining \n",
    "    new_model.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy'])\n",
    "    new_model.summary()\n",
    "    new_model.fit_generator(aug.flow(x_train,y_train_ohe, batch_size=BS),validation_data=(x_val,y_val),\n",
    "    steps_per_epoch=len(x_train) // BS,epochs=50,callbacks=[reduce_lr])\n",
    "    \n",
    "    \n",
    "    \n",
    "    activation_average=[]\n",
    "    for layer in new_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Reshape) and (isinstance(new_model.layers[new_model.layers.index(layer)+1],Multiply)):\n",
    "\n",
    "            test_data=x_test[:1000] #only pick those sample which are correctly classified\n",
    "            true_labels =y_test_ohe[:1000]\n",
    "            predictions = new_model.predict(test_data)\n",
    "            predicted_classes = np.argmax(predictions, axis=1)\n",
    "            true_labels = np.argmax(true_labels, axis=1)\n",
    "            correct_indices = np.where(predicted_classes == true_labels)[0]\n",
    "            correct_test_data = test_data[correct_indices]\n",
    "\n",
    "            # calculating activation for filter pruning\n",
    "\n",
    "            inputs=new_model.input\n",
    "            outputs=layer.output\n",
    "\n",
    "            activation_model=tf.keras.Model(inputs=inputs,outputs=outputs) # create new block for activation calculation\n",
    "            activation_SE=activation_model.predict(correct_test_data)\n",
    "\n",
    "            # calculate the average activation from the SE block, reshape layer is used so we squeeze the shape\n",
    "\n",
    "            acti_average=np.average(np.squeeze(activation_SE),axis=0)\n",
    "            activation_average.append(acti_average)\n",
    "            #activation_average\n",
    "\n",
    "    #delete the model for memory clear up\n",
    "    del new_model\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    return activation_average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.ckpt')\n",
    "#from tensorflow import keras\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "#model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "baseline_model_loss, baseline_model_accuracy = model.evaluate(\n",
    "    x_test,y_test_ohe, verbose=1) \n",
    "\n",
    "print('Baseline test loss:  accuracy:', baseline_model_accuracy)\n",
    "activation_average=[]\n",
    "#activation calculate for filter importance\n",
    "activation_average=AVGSTDGNFilterPruning(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256e42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the file\n",
    "import pickle\n",
    "with open('activation_average_cifar10_layerpruned.pkl', \"rb\") as file:\n",
    "    activation_average = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of values greater than 0.5 for each array\n",
    "percentage_list = []\n",
    "\n",
    "data_list=activation_average\n",
    "for data in data_list:\n",
    "    mean_val = np.mean(data)\n",
    "    num_values_greater_than_0_5 = np.sum(data > 1.2*(mean_val))\n",
    "    total_values = len(data)\n",
    "    percentage = (num_values_greater_than_0_5 / total_values) * 100\n",
    "    percentage_list.append(percentage)\n",
    "\n",
    "# Resulting list of percentages\n",
    "for percentage in percentage_list:\n",
    "    print(f\"Percentage of values over mean: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_pruned_index=np.argsort(layer_rank)[:7]\n",
    "print(layer_pruned_index)\n",
    "#layer_pruned_index=[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea022e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_pruned_index=np.argsort(percentage_list)[:6]\n",
    "print(layer_pruned_index)\n",
    "#layer_pruned_index=[7]\n",
    "layer_pruned_index=[ 9, 10, 11, 12,  8,  7,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fde2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vgg_model_conv_maxpool_index=[0,1,'M',2,3,'M',4,5,6,'M',7,8,9,'M',10,11,12]\n",
    "layer_pruned_index=[ 9,  8, 10, 11,  7,  6, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.ckpt')\n",
    "# Create a new model with  fewer layer\n",
    "def create_new_layerpruned_model(model):\n",
    "    new_layerpruned_model_input = Input(shape=(48, 48, 3))\n",
    "    x =  new_layerpruned_model_input\n",
    "    Conv2d_layer_count=0\n",
    "    for layer in model.layers: \n",
    "        if isinstance(layer, Conv2D):\n",
    "            \n",
    "            if Conv2d_layer_count not in layer_pruned_index:   \n",
    "                x = layer(x)\n",
    "            Conv2d_layer_count+=1\n",
    "            \n",
    "        elif isinstance(layer, MaxPooling2D) :\n",
    "            #x = layer(x)\n",
    "            x=MaxPooling2D(2,2)(x) \n",
    "            \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(512, activation='relu')(x)\n",
    "    x=Dense(512, activation='relu')(x)\n",
    "    x=Dense(10, activation='softmax')(x)\n",
    "    new_layerpruned_model = tf.keras.Model(new_layerpruned_model_input, x)\n",
    "    return new_layerpruned_model\n",
    "\n",
    "new_layerpruned_model = create_new_layerpruned_model(model)\n",
    "new_layerpruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#weight copy from old model to new layer pruned model\n",
    "# we chosse to keep only the first 8 layer of the vgg16 model for histogram\n",
    "\n",
    "Conv2d_layer_count=1\n",
    "for layer,new_layer in zip(model.layers,new_layerpruned_model.layers[1:]):\n",
    "    if isinstance(layer,Conv2D):\n",
    "        new_layer.set_weights(layer.get_weights())\n",
    "        Conv2d_layer_count+=1\n",
    "        if(Conv2d_layer_count>6):\n",
    "           break\n",
    "new_layerpruned_model.layers[-1].set_weights(model.layers[-1].get_weights())\n",
    "new_layerpruned_model.layers[-2].set_weights(model.layers[-2].get_weights())\n",
    "\n",
    "\n",
    "# new model evaluation\n",
    "\n",
    "# initiate SGD optimizer\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * (0.5 ** (epoch // 50))\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "new_layerpruned_model.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy'])\n",
    "     #Finetune\n",
    "\n",
    "    \n",
    "new_layerpruned_model.fit_generator(aug.flow(x_train,y_train_ohe, batch_size=BS),validation_data=(x_val,y_val),\n",
    "    steps_per_epoch=len(x_train) // BS,epochs=1,callbacks=[reduce_lr])\n",
    "\n",
    "baseline_model_loss, new_model_accuracy = new_layerpruned_model.evaluate(\n",
    "    x_test,y_test_ohe, verbose=1) \n",
    "\n",
    "print('new test loss:  accuracy:', new_model_accuracy) \n",
    "\n",
    "print(f\"Total number of parameters before pruning: {model.count_params()}\")\n",
    "print(f\"Total number of parameters after pruning: {new_layerpruned_model.count_params()}\")\n",
    "\n",
    "print(f'Model Flops: {flops_count(model)}')\n",
    "\n",
    "print(f'Layer pruned Model Flops: {flops_count(new_layerpruned_model)}')\n",
    "\n",
    "print(f'Flops reduction:{(1-(flops_count(new_layerpruned_model)/flops_count(model)))*100}%')\n",
    "print(f'Parameters reduction:{(1-(new_layerpruned_model.count_params()/model.count_params()))*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d02c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Model Flops: {flops_count(model)}')\n",
    "def flops_count(new_model):\n",
    "    total_flops = 0\n",
    "    for layer in new_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            #print(f'{layer.name}') \n",
    "            kernel_size = layer.kernel_size\n",
    "            input_channels = layer.input_shape[-1]\n",
    "            output_channels = layer.output_shape[-1]\n",
    "            height = layer.output_shape[1]\n",
    "            width = layer.output_shape[2]\n",
    "            flops = ((kernel_size[0] * kernel_size[1] * input_channels) * output_channels + output_channels) * height * width\n",
    "            total_flops += flops\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            input_neurons = layer.input_shape[-1]\n",
    "            output_neurons = layer.units\n",
    "            flops = (input_neurons * output_neurons) + output_neurons\n",
    "            total_flops += flops\n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2765b35",
   "metadata": {},
   "source": [
    "#  and assign weight from base layer by considering activation average where wheight shape does not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the file\n",
    "with open('activation_average_cifar10_layerpruned.pkl', \"rb\") as file:\n",
    "    activation_average = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8524308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of values greater than 0.5 for each array\n",
    "percentage_list = []\n",
    "\n",
    "data_list=activation_average\n",
    "for data in data_list:\n",
    "    mean_val = np.mean(data)\n",
    "    num_values_greater_than_0_5 = np.sum(data > (1.2*mean_val))\n",
    "    total_values = len(data)\n",
    "    percentage = (num_values_greater_than_0_5 / total_values) * 100\n",
    "    percentage_list.append(percentage)\n",
    "\n",
    "# Resulting list of percentages\n",
    "for percentage in percentage_list:\n",
    "    print(f\"Percentage of values over mean: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3597966",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_pruned_index=np.argsort(percentage_list)[:13]\n",
    "#layer_pruned_index=np.argsort(percentage_list)[::-1][:6]\n",
    "print(layer_pruned_index)\n",
    "#layer_pruned_index=[7]\n",
    "#[ 9  8 10 11  7  6 12 5]\n",
    " 6  5  4  0  1  2  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_pruned_index=[ 9 ,8, 10,11,  7,  12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary with keys and values\n",
    "keys = [0, 1, 'M1', 2, 3, 'M2', 4, 5, 6, 'M3', 7, 8, 9, 'M4', 10, 11, 12,'M5']\n",
    "values = [64, 64, 0, 128, 128, 0, 256, 256, 256, 0, 512, 512, 512, 0, 512, 512, 512,0]\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "# Define the list of indices to set to 0\n",
    "#layer_pruned_index = [9, 3, 10, 11, 7, 6]\n",
    "\n",
    "# Populate the dictionary with key-value pairs\n",
    "for key, value in zip(keys, values):\n",
    "    if key in layer_pruned_index:\n",
    "        result_dict[key] = [value, 0]  # Set to [value, 0] if in layer_pruned_index\n",
    "    else:\n",
    "        result_dict[key] = [value, 1]  # Set to [value, value] if not in layer_pruned_index\n",
    "\n",
    "# Print the updated dictionary\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs= Input(shape=(48, 48, 3))\n",
    "x=inputs\n",
    "maxpool_list=['M1','M2','M3','M4','M5']\n",
    "for key, value in zip(keys, values):\n",
    "  if result_dict[key][1]==1:\n",
    "    if key  not in maxpool_list:\n",
    "      print(result_dict[key][0])\n",
    "      x=Conv2D(result_dict[key][0],kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "    if key   in maxpool_list:\n",
    "      x=MaxPooling2D(2,2)(x)\n",
    "\n",
    "x=Flatten()(x)\n",
    "x=Dense(512, activation='relu')(x)\n",
    "x=Dense(512, activation='relu')(x)\n",
    "x=Dense(100, activation='softmax')(x)\n",
    "new_layerpruned_model = tf.keras.Model(inputs, x)\n",
    "\n",
    "new_layerpruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd545e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef580993",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.ckpt')\n",
    "\n",
    "\n",
    "conv_layer_kept_index=np.sort(np.argsort(percentage_list)[6:])\n",
    "#conv_layer_kept_index=[0,1,2,3,4,5,6]\n",
    "print(conv_layer_kept_index)\n",
    "\n",
    "layer_keeping_index=0\n",
    "model_conv_count=0\n",
    "#enter to the base model to check the kept layer for new model\n",
    "for layer in model.layers:\n",
    "   if isinstance(layer, Conv2D):\n",
    "      if layer_keeping_index in conv_layer_kept_index:\n",
    "         desired_layer=0\n",
    "        # enter the new model to copy the base model weight to new model if layer match in both model\n",
    "         for new_layer in new_layerpruned_model.layers[1:]:\n",
    "            if isinstance(new_layer, Conv2D):\n",
    "            \n",
    "               if model_conv_count==desired_layer:  # check where to assign \n",
    "                  index = np.where(conv_layer_kept_index == layer_keeping_index)[0]\n",
    "                 #in_dex=conv_layer_kept_index.index(layer_keeping_index)-conv_layer_kept_index[index-1]\n",
    "                 #print(index-1)\n",
    "                  if conv_layer_kept_index[index]==0 or conv_layer_kept_index[index]-conv_layer_kept_index[index-1]<=1: # check consecutive layer or not\n",
    "                     #print(layer.get_weights()) \n",
    "                     print(f'No shape change conv layer: {layer_keeping_index}')\n",
    "                     #print(new_layer.get_weights()) \n",
    "                     print('.............................')\n",
    "                     \n",
    "                     new_layer.set_weights(layer.get_weights())\n",
    "                     #print(new_layer.get_weights()) \n",
    "                     break\n",
    "                    \n",
    "                  else:\n",
    "                    # Sort the importance list and get the indices in descending order\n",
    "                     sorted_indices = np.argsort(activation_average[layer_keeping_index])[::-1]\n",
    "                     print(f' Shape change conv layer: {layer_keeping_index}')\n",
    "                    #print(sorted_indices)\n",
    "                     #print(activation_average[layer_keeping_index])\n",
    "                    # Get the weights from the original Conv2D layer\n",
    "                     original_weights = layer.get_weights()\n",
    "                    # Create a new weight matrix with the previous layer  filters by considering activation average\n",
    "                     #print(result_dict[int(conv_layer_kept_index[index-1])][0])\n",
    "                        # only weight depth change to bias\n",
    "                        \n",
    "                     if conv_layer_kept_index[0]!=0 and model_conv_count==0 :# if first layer pruned\n",
    "                        new_depth=3\n",
    "                     else:\n",
    "                        new_depth= result_dict[int(conv_layer_kept_index[index-1])][0]  # remain depth \n",
    "                     print(f'new_depth:{new_depth}')\n",
    "                     print(sorted_indices[:new_depth])\n",
    "                     if new_depth== original_weights[0].shape[2]: # check for same depth ie. layer 5 and 7\n",
    "                        new_layer.set_weights(original_weights)\n",
    "                     else:\n",
    "                        new_weights  = [original_weights[0][:, :,sorted_indices[:new_depth],: ], original_weights[1]]\n",
    "                     # Inspect the shape of the weight arrays\n",
    "                        \n",
    "                     #for i, weight in enumerate(new_weights):\n",
    "                        #print(f\"Weight {i+1} shape: {weight.shape}\")\n",
    "                        new_layer.set_weights(new_weights)\n",
    "                     break\n",
    "               desired_layer=desired_layer+1\n",
    "        \n",
    "        \n",
    "         model_conv_count=model_conv_count+1\n",
    "\n",
    "      layer_keeping_index=layer_keeping_index+1\n",
    "\n",
    "new_layerpruned_model.layers[-1].set_weights(model.layers[-1].get_weights())\n",
    "new_layerpruned_model.layers[-2].set_weights(model.layers[-2].get_weights())\n",
    "#new_layerpruned_model.layers[-3].set_weights(model.layers[-3].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flops_count(new_model):\n",
    "    total_flops = 0\n",
    "    for layer in new_model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            #print(f'{layer.name}') \n",
    "            kernel_size = layer.kernel_size\n",
    "            input_channels = layer.input_shape[-1]\n",
    "            output_channels = layer.output_shape[-1]\n",
    "            height = layer.output_shape[1]\n",
    "            width = layer.output_shape[2]\n",
    "            flops = ((kernel_size[0] * kernel_size[1] * input_channels) * output_channels + output_channels) * height * width\n",
    "            total_flops += flops\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            input_neurons = layer.input_shape[-1]\n",
    "            output_neurons = layer.units\n",
    "            flops = (input_neurons * output_neurons) + output_neurons\n",
    "            total_flops += flops\n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9934c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new model evaluation\n",
    "\n",
    "# initiate SGD optimizer\n",
    "sgd = optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return 0.001 * (0.5 ** (epoch // 50))\n",
    "reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "new_layerpruned_model.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy'])\n",
    "     #Finetune\n",
    "\n",
    "    \n",
    "new_layerpruned_model.fit_generator(aug.flow(x_train,y_train_ohe, batch_size=BS),validation_data=(x_val,y_val),\n",
    "    steps_per_epoch=len(x_train) // BS,epochs=205,callbacks=[reduce_lr])\n",
    "\n",
    "baseline_model_loss, new_model_accuracy = new_layerpruned_model.evaluate(\n",
    "    x_test,y_test_ohe, verbose=1) \n",
    "\n",
    "print('new test loss:  accuracy:', new_model_accuracy) \n",
    "\n",
    "print(f\"Total number of parameters before pruning: {model.count_params()}\")\n",
    "print(f\"Total number of parameters after pruning: {new_layerpruned_model.count_params()}\")\n",
    "\n",
    "print(f'Model Flops: {flops_count(model)}')\n",
    "\n",
    "print(f'Layer pruned Model Flops: {flops_count(new_layerpruned_model)}')\n",
    "\n",
    "print(f'Flops reduction:{(1-(flops_count(new_layerpruned_model)/flops_count(model)))*100}%')\n",
    "print(f'Parameters reduction:{(1-(new_layerpruned_model.count_params()/model.count_params()))*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917267d2",
   "metadata": {},
   "source": [
    "# Inference latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fd0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Prepare a sample input image for inference (adjust shape as needed)\n",
    "input_image = x_test[0:1]  # Taking the first image for inference\n",
    "\n",
    "# Warm-up the model (optional)\n",
    "new_layerpruned_model.predict(input_image)\n",
    "\n",
    "#input_image = x_test[0:100]\n",
    "\n",
    "# Measure inference latency\n",
    "n = 10000  # Number of repetitions\n",
    "total_time = 0\n",
    "\n",
    "for i in range(n):\n",
    "    start_time = time.time()\n",
    "    new_layerpruned_model.predict(x_test[i:i+1])\n",
    "    end_time = time.time()\n",
    "    total_time += (end_time - start_time)\n",
    "\n",
    "average_latency = (total_time / n) *1000\n",
    "print(\"Average Latency:\", average_latency, \"Milli seconds\")\n",
    "print(\"Total Latency:\", total_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1edc4ea",
   "metadata": {},
   "source": [
    "# Linear Classifier Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_classifier_probe_accuracy=[]\n",
    "for linear_probe_position in range(13):\n",
    "    inputs= Input(shape=(48, 48, 3))\n",
    "    x=inputs\n",
    "    count_number_of_conv_in_main_model=0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, MaxPooling2D) :\n",
    "            #x = layer(x)\n",
    "            x=MaxPooling2D(2,2)(x) \n",
    "        elif isinstance(layer,  Conv2D):\n",
    "            x=layer(x)\n",
    "            if linear_probe_position==count_number_of_conv_in_main_model:\n",
    "                break\n",
    "            count_number_of_conv_in_main_model=count_number_of_conv_in_main_model+1\n",
    "            \n",
    "        \n",
    "            \n",
    "    x=Flatten()(x)\n",
    "    x=Dense(512, activation='relu')(x)\n",
    "    x=Dense(512, activation='relu')(x)\n",
    "    x=Dense(100, activation='softmax')(x)\n",
    "    new_linearprobe_model = tf.keras.Model(inputs, x)\n",
    "\n",
    "    new_linearprobe_model.summary()\n",
    "    \n",
    "    #wweight copy\n",
    "    \n",
    "    Conv2d_layer_count=0\n",
    "    for layer,new_layer in zip(model.layers,new_linearprobe_model.layers[1:]):\n",
    "        if isinstance(layer,Conv2D):\n",
    "            new_layer.set_weights(layer.get_weights())\n",
    "            Conv2d_layer_count+=1\n",
    "            if(Conv2d_layer_count>linear_probe_position):\n",
    "               break\n",
    "    \n",
    "    # Determine the index of the layer where you want to start training the last two layers\n",
    "    # For example, if you want to freeze the layers up to the last three layers, use len(model.layers) - 3 as the start_index\n",
    "    start_index = len(new_linearprobe_model.layers) - 3  # Start training from the last two layers\n",
    "\n",
    "    # Freeze layers up to the specified index\n",
    "    for layer in new_linearprobe_model.layers[:start_index]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    \n",
    "        # new model evaluation\n",
    "\n",
    "    # initiate SGD optimizer\n",
    "    sgd = optimizers.SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "    def lr_scheduler(epoch):\n",
    "        return 0.001 * (0.5 ** (epoch // 20))\n",
    "    reduce_lr = LearningRateScheduler(lr_scheduler)\n",
    "    Monitor=[\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=10),\n",
    "    #tf.keras.callbacks.ModelCheckpoint('model.h5',monitor='val_accuracy',mode='max',save_best_only=True)\n",
    "     ]\n",
    "\n",
    "    new_linearprobe_model.compile(loss='categorical_crossentropy',optimizer= sgd,metrics=['accuracy'])\n",
    "         #Finetune\n",
    "\n",
    "\n",
    "    new_linearprobe_model.fit_generator(aug.flow(x_train,y_train_ohe, batch_size=BS),validation_data=(x_val,y_val),\n",
    "        steps_per_epoch=len(x_train) // BS,epochs=100,callbacks=[reduce_lr,Monitor])\n",
    "\n",
    "    baseline_model_loss, new_model_accuracy = new_linearprobe_model.evaluate(\n",
    "        x_test,y_test_ohe, verbose=1) \n",
    "\n",
    "    print(f'Linear classifier probe {linear_probe_position+1} test accuracy:', new_model_accuracy)\n",
    "    \n",
    "    Linear_classifier_probe_accuracy.append(new_model_accuracy)\n",
    "    print(Linear_classifier_probe_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b49aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace this with your list of values)\n",
    "values = [10, 25, 5, 30, 15]\n",
    "\n",
    "# Create bar chart\n",
    "plt.bar(range(1, len(Linear_classifier_probe_accuracy) + 1), Linear_classifier_probe_accuracy)  # Start from 1 and end at len(values)\n",
    "\n",
    "# Add labels to the x-axis (numeric values 1, 2, 3, ...)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.xticks(range(1, len(Linear_classifier_probe_accuracy) + 1))\n",
    "\n",
    "# Add a label to the y-axis (optional)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Add a title (optional)\n",
    "plt.title(\"VGG-16\")\n",
    "\n",
    "# Show the bar chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ad0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data (replace this with your list of values)\n",
    "values = [10, 25, 5, 30, 15]\n",
    "\n",
    "# Create bar chart\n",
    "plt.bar(range(1, len(Linear_classifier_probe_accuracy) + 1), Linear_classifier_probe_accuracy)  # Start from 1 and end at len(values)\n",
    "\n",
    "# Add labels to the x-axis (numeric values 1, 2, 3, ...)\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.xticks(range(1, len(Linear_classifier_probe_accuracy) + 1))\n",
    "\n",
    "# Add a label to the y-axis (optional)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "# Add a title (optional)\n",
    "plt.title(\"VGG-16 \")\n",
    "\n",
    "# Show the bar chart\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
